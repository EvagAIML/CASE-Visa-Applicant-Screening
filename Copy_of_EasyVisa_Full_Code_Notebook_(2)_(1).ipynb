{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EvagAIML/CASE-Visa-Applicant-Screening/blob/main/Copy_of_EasyVisa_Full_Code_Notebook_(2)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "empty-shanghai"
      },
      "source": [
        "### Business Context\n",
        "\n",
        "Businesses in the United States face consistent challenges in sourcing qualified talent.  \n",
        "To fill skill shortages, employers often hire foreign professionals under visa programs governed by the **Immigration and Nationality Act (INA)**.  \n",
        "\n",
        "The **Office of Foreign Labor Certification (OFLC)** reviews and certifies these employer applications when employers demonstrate valid labor shortages and offer competitive wages.  \n",
        "\n",
        "This dataset represents real-world visa applications, each labeled as **CERTIFIED** or **DENIED**.  \n",
        "By analyzing patterns in this data, we aim to predict the likelihood of visa certification and understand which factors most influence these decisions."
      ],
      "id": "empty-shanghai"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem Definition\n",
        "\n",
        "The goal is to build a supervised learning model that predicts whether a visa application will be **CERTIFIED** or **DENIED**.  \n",
        "\n",
        "**Target variable:** `case_status`  \n",
        "**Problem type:** Binary Classification  \n",
        "**Evaluation metrics:** ROC-AUC (primary), F1, Precision, Recall, Accuracy  \n",
        "**Business value:** Early identification of likely certification outcomes can help employers optimize hiring and compliance strategies."
      ],
      "metadata": {
        "id": "wJmgAFK8_aCN"
      },
      "id": "wJmgAFK8_aCN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Description"
      ],
      "metadata": {
        "id": "JFD56lwrCEUo"
      },
      "id": "JFD56lwrCEUo"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 1. LOADING AND SETUP\n",
        "# ===========================\n",
        "\n",
        "# Import basic libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Modeling libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Configure visualization style\n",
        "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
        "\n",
        "# ---------------------------\n",
        "# Load dataset\n",
        "# ---------------------------\n",
        "# Update the file path if running locally\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/EvagAIML/CASE-Visa-Applicant-Screening/refs/heads/main/EasyVisa%20Data%20Science.csv\")\n",
        "\n",
        "# Quick peek at the dataset\n",
        "print(\"Shape of dataset:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "# Basic info and nulls\n",
        "df.info()\n",
        "print(\"\\nMissing values per column:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "id": "KZ3NTOAIvBY2",
        "outputId": "b941966f-9a9d-4315-a507-b64b18236074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        }
      },
      "id": "KZ3NTOAIvBY2",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dataset: (25480, 12)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  case_id continent education_of_employee has_job_experience  \\\n",
              "0  EZYV01      Asia           High School                  N   \n",
              "1  EZYV02      Asia              Master's                  Y   \n",
              "2  EZYV03      Asia            Bachelor's                  N   \n",
              "3  EZYV04      Asia            Bachelor's                  N   \n",
              "4  EZYV05    Africa              Master's                  Y   \n",
              "\n",
              "  requires_job_training  no_of_employees  yr_of_estab region_of_employment  \\\n",
              "0                     N            14513         2007                 West   \n",
              "1                     N             2412         2002            Northeast   \n",
              "2                     Y            44444         2008                 West   \n",
              "3                     N               98         1897                 West   \n",
              "4                     N             1082         2005                South   \n",
              "\n",
              "   prevailing_wage unit_of_wage full_time_position case_status  \n",
              "0         592.2029         Hour                  Y      Denied  \n",
              "1       83425.6500         Year                  Y   Certified  \n",
              "2      122996.8600         Year                  Y      Denied  \n",
              "3       83434.0300         Year                  Y      Denied  \n",
              "4      149907.3900         Year                  Y   Certified  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae7414a2-ac51-4f2b-8563-14b24f425cd6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>case_id</th>\n",
              "      <th>continent</th>\n",
              "      <th>education_of_employee</th>\n",
              "      <th>has_job_experience</th>\n",
              "      <th>requires_job_training</th>\n",
              "      <th>no_of_employees</th>\n",
              "      <th>yr_of_estab</th>\n",
              "      <th>region_of_employment</th>\n",
              "      <th>prevailing_wage</th>\n",
              "      <th>unit_of_wage</th>\n",
              "      <th>full_time_position</th>\n",
              "      <th>case_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>EZYV01</td>\n",
              "      <td>Asia</td>\n",
              "      <td>High School</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>14513</td>\n",
              "      <td>2007</td>\n",
              "      <td>West</td>\n",
              "      <td>592.2029</td>\n",
              "      <td>Hour</td>\n",
              "      <td>Y</td>\n",
              "      <td>Denied</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EZYV02</td>\n",
              "      <td>Asia</td>\n",
              "      <td>Master's</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>2412</td>\n",
              "      <td>2002</td>\n",
              "      <td>Northeast</td>\n",
              "      <td>83425.6500</td>\n",
              "      <td>Year</td>\n",
              "      <td>Y</td>\n",
              "      <td>Certified</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EZYV03</td>\n",
              "      <td>Asia</td>\n",
              "      <td>Bachelor's</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>44444</td>\n",
              "      <td>2008</td>\n",
              "      <td>West</td>\n",
              "      <td>122996.8600</td>\n",
              "      <td>Year</td>\n",
              "      <td>Y</td>\n",
              "      <td>Denied</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>EZYV04</td>\n",
              "      <td>Asia</td>\n",
              "      <td>Bachelor's</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>98</td>\n",
              "      <td>1897</td>\n",
              "      <td>West</td>\n",
              "      <td>83434.0300</td>\n",
              "      <td>Year</td>\n",
              "      <td>Y</td>\n",
              "      <td>Denied</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>EZYV05</td>\n",
              "      <td>Africa</td>\n",
              "      <td>Master's</td>\n",
              "      <td>Y</td>\n",
              "      <td>N</td>\n",
              "      <td>1082</td>\n",
              "      <td>2005</td>\n",
              "      <td>South</td>\n",
              "      <td>149907.3900</td>\n",
              "      <td>Year</td>\n",
              "      <td>Y</td>\n",
              "      <td>Certified</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae7414a2-ac51-4f2b-8563-14b24f425cd6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae7414a2-ac51-4f2b-8563-14b24f425cd6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae7414a2-ac51-4f2b-8563-14b24f425cd6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-394f8fd8-259a-4e7f-bdf0-2279890a4207\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-394f8fd8-259a-4e7f-bdf0-2279890a4207')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-394f8fd8-259a-4e7f-bdf0-2279890a4207 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\nMissing values per column:\\\\n\\\", df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"case_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"EZYV02\",\n          \"EZYV05\",\n          \"EZYV03\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"continent\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Africa\",\n          \"Asia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"education_of_employee\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"High School\",\n          \"Master's\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"has_job_experience\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Y\",\n          \"N\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"requires_job_training\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Y\",\n          \"N\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"no_of_employees\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18777,\n        \"min\": 98,\n        \"max\": 44444,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2412,\n          1082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"yr_of_estab\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 48,\n        \"min\": 1897,\n        \"max\": 2008,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2002,\n          2005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"region_of_employment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"West\",\n          \"Northeast\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prevailing_wage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56433.97809427707,\n        \"min\": 592.2029,\n        \"max\": 149907.39,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          83425.65,\n          149907.39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"unit_of_wage\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Year\",\n          \"Hour\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"full_time_position\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Y\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"case_status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Certified\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25480 entries, 0 to 25479\n",
            "Data columns (total 12 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   case_id                25480 non-null  object \n",
            " 1   continent              25480 non-null  object \n",
            " 2   education_of_employee  25480 non-null  object \n",
            " 3   has_job_experience     25480 non-null  object \n",
            " 4   requires_job_training  25480 non-null  object \n",
            " 5   no_of_employees        25480 non-null  int64  \n",
            " 6   yr_of_estab            25480 non-null  int64  \n",
            " 7   region_of_employment   25480 non-null  object \n",
            " 8   prevailing_wage        25480 non-null  float64\n",
            " 9   unit_of_wage           25480 non-null  object \n",
            " 10  full_time_position     25480 non-null  object \n",
            " 11  case_status            25480 non-null  object \n",
            "dtypes: float64(1), int64(2), object(9)\n",
            "memory usage: 2.3+ MB\n",
            "\n",
            "Missing values per column:\n",
            " case_id                  0\n",
            "continent                0\n",
            "education_of_employee    0\n",
            "has_job_experience       0\n",
            "requires_job_training    0\n",
            "no_of_employees          0\n",
            "yr_of_estab              0\n",
            "region_of_employment     0\n",
            "prevailing_wage          0\n",
            "unit_of_wage             0\n",
            "full_time_position       0\n",
            "case_status              0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary: Loading and Setup\n",
        "\n",
        "The dataset was successfully loaded from GitHub into a pandas DataFrame containing 25,480 records and 12 columns. It includes both categorical and numerical variables with appropriate data types and no missing values. The data appears complete and consistent, making it ready for further analysis and integrity validation."
      ],
      "metadata": {
        "id": "7_N4cbMd2jEc"
      },
      "id": "7_N4cbMd2jEc"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qnBe4X56rN-C"
      },
      "id": "qnBe4X56rN-C",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Understanding\n",
        "\n",
        "Before transformation or modeling, the dataset’s integrity is checked.  \n",
        "This includes verifying record and feature counts, column types, missing values, duplicates, unique counts, and logical consistency (e.g., no negative employee counts or invalid years)."
      ],
      "metadata": {
        "id": "uae5fMIyAnd-"
      },
      "id": "uae5fMIyAnd-"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Data Understanding — Integrity Checks (read-only)\n",
        "# ===============================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 0) Preconditions\n",
        "assert 'df' in globals() and isinstance(df, pd.DataFrame), \"DataFrame `df` is not loaded.\"\n",
        "\n",
        "# 1) Shape and basic info\n",
        "print(\"=== Dataset Shape ===\")\n",
        "rows, cols = df.shape\n",
        "print(f\"Records: {rows}, Features: {cols}\\n\")\n",
        "\n",
        "print(\"=== Column Information ===\")\n",
        "df.info()\n",
        "print()\n",
        "\n",
        "# 2) Missing values\n",
        "print(\"=== Missing Values per Column ===\")\n",
        "missing = df.isnull().sum()\n",
        "total_missing = int(missing.sum())\n",
        "if total_missing == 0:\n",
        "    print(\"No missing values detected.\\n\")\n",
        "else:\n",
        "    print(f\"Total missing values: {total_missing}\")\n",
        "    print(missing[missing > 0].to_string(), \"\\n\")\n",
        "\n",
        "# 3) Duplicate rows\n",
        "print(\"=== Duplicate Records ===\")\n",
        "dup_count = int(df.duplicated().sum())\n",
        "print(f\"Duplicate rows: {dup_count}\\n\")\n",
        "\n",
        "# 4) Data types and unique counts\n",
        "print(\"=== Column Data Types ===\")\n",
        "print(df.dtypes.to_string(), \"\\n\")\n",
        "\n",
        "print(\"=== Unique Value Counts per Column ===\")\n",
        "print(df.nunique().to_string(), \"\\n\")\n",
        "\n",
        "# 5) Logical consistency checks\n",
        "print(\"=== Logical Consistency Checks ===\")\n",
        "\n",
        "# 5a) Negative values in numeric columns\n",
        "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "if num_cols:\n",
        "    neg_counts = (df[num_cols] < 0).sum()\n",
        "    neg_counts = neg_counts[neg_counts > 0]\n",
        "    if not neg_counts.empty:\n",
        "        print(\"Negative values detected:\")\n",
        "        print(neg_counts.to_string())\n",
        "    else:\n",
        "        print(\"No negative values detected in numeric columns.\")\n",
        "else:\n",
        "    print(\"No numeric columns detected.\")\n",
        "\n",
        "# 5b) Establishment year plausibility (if present)\n",
        "if 'yr_of_estab' in df.columns:\n",
        "    yrs = pd.to_numeric(df['yr_of_estab'], errors='coerce')\n",
        "    invalid_year_mask = (yrs < 1800) | (yrs > 2025) | yrs.isna()\n",
        "    invalid_years = int(invalid_year_mask.sum())\n",
        "    print(f\"\\nyr_of_estab outside [1800, 2025] or non-parsable: {invalid_years}\")\n",
        "else:\n",
        "    print(\"\\nColumn `yr_of_estab` not present; year plausibility check skipped.\")\n",
        "\n",
        "# 6) Target distribution (if present)\n",
        "if 'case_status' in df.columns:\n",
        "    print(\"\\n=== Target Variable (case_status) Distribution ===\")\n",
        "    vc = df['case_status'].value_counts(dropna=False)\n",
        "    pct = (vc / len(df) * 100).round(2)\n",
        "    print(pd.DataFrame({'Count': vc, 'Percent': pct}).to_string(index=True))\n",
        "else:\n",
        "    print(\"\\nTarget column `case_status` not found.\")\n",
        "\n",
        "# 7) Summary line\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(f\"- Total records: {rows}\")\n",
        "print(f\"- Total features: {cols}\")\n",
        "print(f\"- Missing values: {total_missing}\")\n",
        "print(f\"- Duplicate rows: {dup_count}\")\n",
        "if num_cols:\n",
        "    neg_total = int(((df[num_cols] < 0).sum()).sum())\n",
        "    print(f\"- Total negative numeric entries: {neg_total}\")\n",
        "if 'yr_of_estab' in df.columns:\n",
        "    print(f\"- Invalid or out-of-range `yr_of_estab` rows: {invalid_years}\")\n",
        "print(\"Integrity check complete.\")"
      ],
      "metadata": {
        "id": "JATCKi4oHfRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32dc1476-3706-4955-a226-ce7af98e3037"
      },
      "id": "JATCKi4oHfRO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dataset Shape ===\n",
            "Records: 25480, Features: 12\n",
            "\n",
            "=== Column Information ===\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25480 entries, 0 to 25479\n",
            "Data columns (total 12 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   case_id                25480 non-null  object \n",
            " 1   continent              25480 non-null  object \n",
            " 2   education_of_employee  25480 non-null  object \n",
            " 3   has_job_experience     25480 non-null  object \n",
            " 4   requires_job_training  25480 non-null  object \n",
            " 5   no_of_employees        25480 non-null  int64  \n",
            " 6   yr_of_estab            25480 non-null  int64  \n",
            " 7   region_of_employment   25480 non-null  object \n",
            " 8   prevailing_wage        25480 non-null  float64\n",
            " 9   unit_of_wage           25480 non-null  object \n",
            " 10  full_time_position     25480 non-null  object \n",
            " 11  case_status            25480 non-null  object \n",
            "dtypes: float64(1), int64(2), object(9)\n",
            "memory usage: 2.3+ MB\n",
            "\n",
            "=== Missing Values per Column ===\n",
            "No missing values detected.\n",
            "\n",
            "=== Duplicate Records ===\n",
            "Duplicate rows: 0\n",
            "\n",
            "=== Column Data Types ===\n",
            "case_id                   object\n",
            "continent                 object\n",
            "education_of_employee     object\n",
            "has_job_experience        object\n",
            "requires_job_training     object\n",
            "no_of_employees            int64\n",
            "yr_of_estab                int64\n",
            "region_of_employment      object\n",
            "prevailing_wage          float64\n",
            "unit_of_wage              object\n",
            "full_time_position        object\n",
            "case_status               object \n",
            "\n",
            "=== Unique Value Counts per Column ===\n",
            "case_id                  25480\n",
            "continent                    6\n",
            "education_of_employee        4\n",
            "has_job_experience           2\n",
            "requires_job_training        2\n",
            "no_of_employees           7105\n",
            "yr_of_estab                199\n",
            "region_of_employment         5\n",
            "prevailing_wage          25454\n",
            "unit_of_wage                 4\n",
            "full_time_position           2\n",
            "case_status                  2 \n",
            "\n",
            "=== Logical Consistency Checks ===\n",
            "Negative values detected:\n",
            "no_of_employees    33\n",
            "\n",
            "yr_of_estab outside [1800, 2025] or non-parsable: 0\n",
            "\n",
            "=== Target Variable (case_status) Distribution ===\n",
            "             Count  Percent\n",
            "case_status                \n",
            "Certified    17018    66.79\n",
            "Denied        8462    33.21\n",
            "\n",
            "=== Summary ===\n",
            "- Total records: 25480\n",
            "- Total features: 12\n",
            "- Missing values: 0\n",
            "- Duplicate rows: 0\n",
            "- Total negative numeric entries: 33\n",
            "- Invalid or out-of-range `yr_of_estab` rows: 0\n",
            "Integrity check complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary: Data Understanding\n",
        "\n",
        "A detailed integrity check was performed to confirm the dataset’s completeness and logical consistency before any transformation or modeling. The dataset contains 25,480 records and 12 features, with no missing values or duplicate entries. Data types were validated, consisting of nine categorical and three numerical variables.\n",
        "\n",
        "The analysis identified 33 negative values in the no_of_employees column, which will require correction before modeling. All establishment years (yr_of_estab) fall within the valid range of 1800–2025, indicating no temporal inconsistencies. The target variable case_status shows a binary distribution of approximately 67% Certified and 33% Denied cases, suggesting a moderate class imbalance."
      ],
      "metadata": {
        "id": "LXtZ76sS6MmR"
      },
      "id": "LXtZ76sS6MmR"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fjvj9HtB3uau"
      },
      "id": "Fjvj9HtB3uau",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Primary Data Cleaning\n",
        "\n",
        "Only essential corrections are made before data splitting to preserve integrity. Negative employee counts are corrected, establishment years are validated, and categorical fields are standardized for consistency. Wage units are checked for uniformity, and non-informative identifiers are reviewed for removal. This ensures the dataset is accurate and consistent while minimizing the risk of data distortion."
      ],
      "metadata": {
        "id": "qDywS1c9_iaY"
      },
      "id": "qDywS1c9_iaY"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Primary Data Cleaning (essential fixes only)\n",
        "# - Non-destructive to `df` (creates `df_clean`)\n",
        "# - Minimal, pre-split corrections per EasyVisa\n",
        "# ===========================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "assert 'df' in globals() and isinstance(df, pd.DataFrame), \"`df` is not loaded.\"\n",
        "\n",
        "# Work on a copy (do not mutate original)\n",
        "df_clean = df.copy()\n",
        "\n",
        "rows_before = len(df_clean)\n",
        "reports = []\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Normalize categorical fields (minimal)\n",
        "#    - Trim whitespace\n",
        "#    - Standardize known Y/N fields to YES/NO\n",
        "#    - Uppercase other text for consistency\n",
        "# ---------------------------\n",
        "yn_cols = [c for c in ['has_job_experience', 'requires_job_training', 'full_time_position'] if c in df_clean.columns]\n",
        "\n",
        "def to_yes_no(x):\n",
        "    s = str(x).strip().lower()\n",
        "    yes_set = {'y','yes','true','t','1'}\n",
        "    no_set  = {'n','no','false','f','0'}\n",
        "    if s in yes_set: return 'YES'\n",
        "    if s in no_set:  return 'NO'\n",
        "    return str(x).strip().upper()  # leave as-is but normalized\n",
        "\n",
        "# Trim and normalize objects\n",
        "for c in df_clean.select_dtypes(include='object').columns:\n",
        "    if c in yn_cols:\n",
        "        df_clean[c] = df_clean[c].apply(to_yes_no)\n",
        "    else:\n",
        "        df_clean[c] = df_clean[c].astype(str).str.strip().str.upper()\n",
        "\n",
        "reports.append(f\"Text normalization applied to object columns; Y/N fields standardized: {', '.join(yn_cols) if yn_cols else 'none'}.\")\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Fix negative employee counts (replace with median of valid)\n",
        "# ---------------------------\n",
        "neg_fixed = 0\n",
        "if 'no_of_employees' in df_clean.columns:\n",
        "    emp = pd.to_numeric(df_clean['no_of_employees'], errors='coerce')\n",
        "    neg_mask = emp < 0\n",
        "    neg_fixed = int(neg_mask.sum())\n",
        "    if neg_fixed > 0:\n",
        "        med_emp = emp[~neg_mask].median()\n",
        "        df_clean.loc[neg_mask, 'no_of_employees'] = med_emp\n",
        "reports.append(f\"Negative 'no_of_employees' corrected: {neg_fixed}.\")\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Validate establishment year (keep realistic range 1800–2025)\n",
        "#    - Remove rows with out-of-range or non-parsable years\n",
        "# ---------------------------\n",
        "years_dropped = 0\n",
        "if 'yr_of_estab' in df_clean.columns:\n",
        "    yrs = pd.to_numeric(df_clean['yr_of_estab'], errors='coerce')\n",
        "    bad_year = yrs.lt(1800) | yrs.gt(2025) | yrs.isna()\n",
        "    years_dropped = int(bad_year.sum())\n",
        "    if years_dropped > 0:\n",
        "        df_clean = df_clean.loc[~bad_year].copy()\n",
        "reports.append(f\"Rows removed for invalid 'yr_of_estab': {years_dropped}.\")\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Wage unit standardization (create yearly metric)\n",
        "#    - Do NOT overwrite original; add 'prevailing_wage_yearly'\n",
        "# ---------------------------\n",
        "unknown_units = 0\n",
        "created_wage_yearly = False\n",
        "if {'prevailing_wage', 'unit_of_wage'}.issubset(df_clean.columns):\n",
        "    u = df_clean['unit_of_wage'].astype(str).str.upper().str.strip()\n",
        "    # Common variants mapped to annualization factors\n",
        "    factors = {\n",
        "        'HOUR': 2080, 'HOURLY': 2080, 'HR': 2080, 'HRS': 2080,\n",
        "        'WEEK': 52, 'WEEKLY': 52,\n",
        "        'MONTH': 12, 'MONTHLY': 12,\n",
        "        'YEAR': 1, 'YEARLY': 1, 'ANNUAL': 1, 'PER YEAR': 1\n",
        "    }\n",
        "    factor = u.map(factors)\n",
        "    unknown_units = int(factor.isna().sum())\n",
        "    factor = factor.fillna(1)  # fallback: assume already annual if unknown\n",
        "    w = pd.to_numeric(df_clean['prevailing_wage'], errors='coerce')\n",
        "    df_clean['prevailing_wage_yearly'] = w * factor\n",
        "    created_wage_yearly = True\n",
        "reports.append(f\"'prevailing_wage_yearly' created: {created_wage_yearly} (unknown units encountered: {unknown_units}).\")\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Drop non-informative ID columns (only if strictly unique)\n",
        "# ---------------------------\n",
        "dropped_ids = []\n",
        "for col in ['case_id', 'id', 'caseid']:\n",
        "    if col in df_clean.columns:\n",
        "        # Drop only if it's a pure identifier (all or near-all unique)\n",
        "        if df_clean[col].nunique(dropna=False) >= 0.99 * len(df_clean):\n",
        "            df_clean.drop(columns=[col], inplace=True)\n",
        "            dropped_ids.append(col)\n",
        "reports.append(f\"ID columns dropped (uniqueness-based): {', '.join(dropped_ids) if dropped_ids else 'none'}.\")\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Report\n",
        "# ---------------------------\n",
        "rows_after = len(df_clean)\n",
        "\n",
        "print(\"=== Primary Data Cleaning (Essential, Pre-Split) ===\")\n",
        "print(f\"Rows before: {rows_before} | Rows after: {rows_after} | Net change: {rows_after - rows_before}\")\n",
        "for r in reports:\n",
        "    print(\"-\", r)\n",
        "\n",
        "# Quick sanity: no mutation of original dataframe\n",
        "assert len(df) == rows_before, \"Original `df` row count changed — this step should not mutate `df`.\"\n",
        "\n",
        "# df_clean is the cleaned dataset to use in subsequent steps (EDA → split → prep → modeling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnRGnvSlAqA-",
        "outputId": "631b90e2-2e05-46d0-f73c-29e6d8b5d3e4"
      },
      "id": "ZnRGnvSlAqA-",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Primary Data Cleaning (Essential, Pre-Split) ===\n",
            "Rows before: 25480 | Rows after: 25480 | Net change: 0\n",
            "- Text normalization applied to object columns; Y/N fields standardized: has_job_experience, requires_job_training, full_time_position.\n",
            "- Negative 'no_of_employees' corrected: 33.\n",
            "- Rows removed for invalid 'yr_of_estab': 0.\n",
            "- 'prevailing_wage_yearly' created: True (unknown units encountered: 0).\n",
            "- ID columns dropped (uniqueness-based): case_id.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary: Primary Data Cleaning\n",
        "\n",
        "Essential cleaning was completed to ensure the EasyVisa dataset was logically consistent before splitting. Text fields were standardized, and categorical “Yes/No” variables were normalized for uniformity. Thirty-three negative employee counts were corrected using the median of valid values, while all establishment years were confirmed to be within realistic bounds (1800–2025). A new prevailing_wage_yearly feature was created to standardize wage comparisons across records, and the non-informative identifier case_id was removed.\n",
        "\n",
        "The dataset retained all 25,480 records, confirming that only minimal, necessary corrections were applied. This ensures data integrity while preparing the dataset for exploratory analysis and model development."
      ],
      "metadata": {
        "id": "FtwbUJYQBK_f"
      },
      "id": "FtwbUJYQBK_f"
    },
    {
      "cell_type": "code",
      "source": [
        "1. Problem Definition ✅\n",
        "2. Data Loading and Setup ✅\n",
        "3. Data Understanding ✅\n",
        "4. Data Cleaning ✅\n",
        "5. Exploratory Data Analysis (Univariate + Bivariate)\n",
        "6. Feature Engineering (Design)\n",
        "7. **Split Data (Train/Validation/Test)**\n",
        "8. Data Preparation (Encoding, Scaling, SMOTE)\n",
        "9. Model Building\n",
        "10. Hyperparameter Tuning\n",
        "11. Model Testing\n",
        "12. Interpretation and Insights\n",
        "13. Summary and Recommendations"
      ],
      "metadata": {
        "id": "X6exSRDI_nqv"
      },
      "id": "X6exSRDI_nqv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4Cw7f9dBPCG"
      },
      "id": "O4Cw7f9dBPCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gn3WYvYKBO9z"
      },
      "id": "Gn3WYvYKBO9z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-cC0hE6_BO5X"
      },
      "id": "-cC0hE6_BO5X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PtzKPZVGBO2g"
      },
      "id": "PtzKPZVGBO2g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3M-qkp8zBOvv"
      },
      "id": "3M-qkp8zBOvv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R32PwDq-BOth"
      },
      "id": "R32PwDq-BOth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BK7hdaBBOrT"
      },
      "id": "2BK7hdaBBOrT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yg2Iq5OIBOo1"
      },
      "id": "yg2Iq5OIBOo1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LbRwHBEyBOkz"
      },
      "id": "LbRwHBEyBOkz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbX99DE7BOgw"
      },
      "id": "xbX99DE7BOgw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UWNWAqsBBOdl"
      },
      "id": "UWNWAqsBBOdl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Coding\n",
        "\n",
        "During the execution phase (after split):\n",
        "\n",
        "Fit encoders, imputers, and scalers on the training data only\n",
        "\n",
        "Transform both training and test sets using the trained encoders\n",
        "\n",
        "Integrate transformations into pipelines for cross-validation and tuning\n",
        "\n",
        "In short, low-risk feature coding (planning) happens before **the** split, while high-risk coding (fitting) happens afterward, maintaining both generalizability and fairness."
      ],
      "metadata": {
        "id": "5JkgLdYx_f5a"
      },
      "id": "5JkgLdYx_f5a"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JqQ1fQLfHdoW"
      },
      "id": "JqQ1fQLfHdoW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (Univariate & Bivariate)"
      ],
      "metadata": {
        "id": "TkwMFns3GYIg"
      },
      "id": "TkwMFns3GYIg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having confirmed the dataset’s integrity and planned for feature encoding, the next step is to understand each variable individually (univariate analysis) and its relationship with the target variable (bivariate analysis).\n",
        "\n",
        "These two steps together allow us to uncover distribution patterns, imbalances, and predictive relationships — guiding cleaning decisions and model selection."
      ],
      "metadata": {
        "id": "uuAZlHg-GqvN"
      },
      "id": "uuAZlHg-GqvN"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ki6W1kAqHVeK"
      },
      "id": "ki6W1kAqHVeK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate analysis\n",
        "\n",
        "Univariate analysis examines each variable independently to understand its type, range, and shape of distribution.\n",
        "\n",
        "For numerical features, we look at central tendency, spread, and outliers using histograms and boxplots.\n",
        "\n",
        "For categorical features, we examine frequency distributions to identify imbalance or dominant classes."
      ],
      "metadata": {
        "id": "e9OcXUqQHEbZ"
      },
      "id": "e9OcXUqQHEbZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Robust CSV loader with diagnostics for GitHub/Colab ===\n",
        "import os, re, sys, io, textwrap\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "def diagnose_and_load_csv(url, local_name=\"dataset.csv\"):\n",
        "    print(f\"Attempting download:\\n{url}\\n\")\n",
        "\n",
        "    # 1) Download\n",
        "    try:\n",
        "        r = requests.get(url, timeout=30)\n",
        "        print(f\"HTTP status: {r.status_code}\")\n",
        "        ctype = r.headers.get(\"Content-Type\", \"?\")\n",
        "        print(f\"Content-Type: {ctype}\")\n",
        "    except Exception as e:\n",
        "        print(\"Download failed:\", e)\n",
        "        raise\n",
        "\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(\"Non-200 response. This is usually a wrong URL or a private file.\")\n",
        "\n",
        "    text = r.text\n",
        "\n",
        "    # 2) Quick sanity: are we accidentally getting HTML (GitHub page instead of raw)?\n",
        "    head = text[:200].strip()\n",
        "    if head.startswith(\"<!DOCTYPE\") or \"<html\" in head.lower():\n",
        "        preview = textwrap.shorten(head.replace(\"\\n\", \" \"), width=200)\n",
        "        raise RuntimeError(\n",
        "            \"Received HTML instead of CSV (likely the repo page, not the RAW file).\\n\"\n",
        "            \"Click the 'Raw' button in GitHub and use THAT URL.\\n\"\n",
        "            f\"First 200 chars:\\n{preview}\"\n",
        "        )\n",
        "\n",
        "    # 3) Save a copy so pandas can stream efficiently\n",
        "    with open(local_name, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        f.write(text)\n",
        "    print(f\"Saved to: {os.path.abspath(local_name)}\\n\")\n",
        "\n",
        "    # 4) Try flexible parses\n",
        "    #    - sep=None -> pandas infers delimiter\n",
        "    #    - engine='python' is more forgiving\n",
        "    #    - low_memory=False avoids mixed-type chunk guessing issues\n",
        "    #    - try utf-8 first, then latin-1\n",
        "    parse_attempts = [\n",
        "        dict(encoding=\"utf-8\", sep=None, engine=\"python\", low_memory=False),\n",
        "        dict(encoding=\"latin-1\", sep=None, engine=\"python\", low_memory=False),\n",
        "    ]\n",
        "\n",
        "    last_err = None\n",
        "    for i, opts in enumerate(parse_attempts, 1):\n",
        "        try:\n",
        "            print(f\"Parse attempt {i} with options: {opts}\")\n",
        "            df = pd.read_csv(local_name, **opts)\n",
        "            print(\"Parse succeeded.\\n\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Parse attempt {i} failed: {e}\\n\")\n",
        "            last_err = e\n",
        "            df = None\n",
        "\n",
        "    if df is None:\n",
        "        raise last_err\n",
        "\n",
        "    # 5) Basic diagnostics\n",
        "    print(\"Loaded DataFrame summary:\")\n",
        "    print(\"Shape:\", df.shape)\n",
        "    print(\"Columns:\", list(df.columns))\n",
        "    print(df.head(3))\n",
        "\n",
        "    # 6) Optional: clean likely numeric columns that may contain commas/$\n",
        "    #    (Uncomment if you see strings in numeric fields like 'prevailing_wage')\n",
        "    likely_numeric = [\"prevailing_wage\", \"no_of_employees\", \"yr_of_estab\"]\n",
        "    for col in likely_numeric:\n",
        "        if col in df.columns and df[col].dtype == \"object\":\n",
        "            cleaned = (\n",
        "                df[col]\n",
        "                .astype(str)\n",
        "                .str.replace(r\"[$,]\", \"\", regex=True)\n",
        "                .str.strip()\n",
        "            )\n",
        "            # convert where possible, leave others as NaN\n",
        "            df[col] = pd.to_numeric(cleaned, errors=\"coerce\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ==== SET YOUR RAW CSV URL HERE ====\n",
        "# Tip: open the file on GitHub, click \"Raw\", copy THAT URL.\n",
        "url = \"https://raw.githubusercontent.com/EvagAIML/CASE-Visa-Applicant-Screening/main/EasyVisa.csv\"\n",
        "\n",
        "visa_df = diagnose_and_load_csv(url, local_name=\"EasyVisa.csv\")\n",
        "\n",
        "print(\"\\nFinal check:\")\n",
        "print(\"visa_df shape:\", visa_df.shape)\n",
        "print(visa_df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "2iTG00cbGBr2",
        "outputId": "9a61c5dc-bb7a-434c-eba0-fc5b2a9c9935"
      },
      "id": "2iTG00cbGBr2",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting download:\n",
            "https://raw.githubusercontent.com/EvagAIML/CASE-Visa-Applicant-Screening/main/EasyVisa.csv\n",
            "\n",
            "HTTP status: 404\n",
            "Content-Type: text/plain; charset=utf-8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Non-200 response. This is usually a wrong URL or a private file.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4224756964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/EvagAIML/CASE-Visa-Applicant-Screening/main/EasyVisa.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mvisa_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiagnose_and_load_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"EasyVisa.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFinal check:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4224756964.py\u001b[0m in \u001b[0;36mdiagnose_and_load_csv\u001b[0;34m(url, local_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Non-200 response. This is usually a wrong URL or a private file.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Non-200 response. This is usually a wrong URL or a private file."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RsMl9FhTFq6p"
      },
      "id": "RsMl9FhTFq6p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bivariate Analysis"
      ],
      "metadata": {
        "id": "0yFY8icE_c97"
      },
      "id": "0yFY8icE_c97"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bivariate analysis explores how each feature relates to the target variable (case_status), identifying variables that may influence the outcome.\n",
        "\n",
        "Numeric vs Target: Boxplots reveal how numeric features differ between approved and denied applications.\n",
        "\n",
        "Categorical vs Target: Cross-tabulations and count plots show class-wise distributions."
      ],
      "metadata": {
        "id": "kYyz_73-_a8X"
      },
      "id": "kYyz_73-_a8X"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# Bivariate Analysis — Relationship with Target (case_status)\n",
        "# ===========================================================\n",
        "\n",
        "target_col = 'case_status'\n",
        "\n",
        "if target_col in df.columns:\n",
        "    # --- Numeric vs Target ---\n",
        "    for col in num_cols:\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.boxplot(x=target_col, y=col, data=df)\n",
        "        plt.title(f\"{col} vs {target_col}\")\n",
        "        plt.xlabel(\"Case Status\")\n",
        "        plt.ylabel(col)\n",
        "        plt.show()\n",
        "\n",
        "    # --- Categorical vs Target ---\n",
        "    for col in cat_cols:\n",
        "        if col != target_col:\n",
        "            cross_tab = pd.crosstab(df[col], df[target_col], normalize='index') * 100\n",
        "            print(f\"\\n{col} vs {target_col} (percentage breakdown):\")\n",
        "            display(cross_tab.round(2))\n",
        "\n",
        "            plt.figure(figsize=(7, 4))\n",
        "            sns.countplot(x=col, hue=target_col, data=df)\n",
        "            plt.title(f\"{col} vs Case Status\")\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel(\"Count\")\n",
        "            plt.xticks(rotation=30)\n",
        "            plt.show()\n",
        "else:\n",
        "    print(\"Target variable `case_status` not found.\")"
      ],
      "metadata": {
        "id": "XiIWW2QGKARD",
        "outputId": "3498313e-0e85-4bcb-e143-d5104ed1f1c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "id": "XiIWW2QGKARD",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_cols' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3306266697.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# --- Numeric vs Target ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_cols' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V9HvcscMOMHG"
      },
      "id": "V9HvcscMOMHG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Insights\n",
        "\n",
        "Univariate analysis reveals how features vary individually — showing data spread, outliers, and category dominance.\n",
        "Bivariate analysis highlights relationships with case_status and helps identify the most discriminative features.\n",
        "\n",
        "Together, these analyses:\n",
        "\n",
        "Confirm data balance or imbalance in key categories (e.g., approval vs denial).\n",
        "\n",
        "Reveal numeric variables with possible outliers or skewness.\n",
        "\n",
        "Show which categorical variables (e.g., education, region, experience) may influence visa outcomes.\n",
        "\n",
        "These insights directly inform data cleaning, feature engineering, and model selection steps that follow.\n"
      ],
      "metadata": {
        "id": "fF7RAIXlJ_ez"
      },
      "id": "fF7RAIXlJ_ez"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dQ0mZn9O9FSP"
      },
      "id": "dQ0mZn9O9FSP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Discovery & Controlled Cleaning (Pre-Split Phase)"
      ],
      "metadata": {
        "id": "TFuLV0rbK6Lp"
      },
      "id": "TFuLV0rbK6Lp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following univariate and bivariate analysis, several controlled data corrections are necessary before model preparation.\n",
        "These corrections are guided by logical rules rather than learned transformations to prevent data leakage.\n",
        "The objective is to create a consistent, error-free dataset without altering its natural distribution.\n",
        "\n",
        "What is done in this phase:\n",
        "\n",
        "\n",
        "**Issue**\n",
        "Negative or invalid values\n",
        "\n",
        "**Column(s)**\n",
        "no_of_employees\n",
        "\n",
        "**Action**\n",
        "Replace negative values with median of valid entries\n",
        "\n",
        "**Rational**\n",
        "Logical correction (counts cannot be negative)\n",
        "\n",
        "\n",
        "\n",
        "**Issue**\n",
        "Implausible establishment years\n",
        "\n",
        "**Column(s)**\n",
        "yr_of_estab\n",
        "\n",
        "**Action**\n",
        "Remove or flag years outside 1800–2025\n",
        "\n",
        "**Rational**\n",
        "Temporal validity check\n",
        "\n",
        "\n",
        "**Issue**\n",
        "Unit inconsistency\n",
        "\n",
        "**Column(s)**\n",
        "unit_of_wage\n",
        "\n",
        "**Action**\n",
        "Convert all wages to yearly equivalent\n",
        "\n",
        "**Rational**\n",
        "Standardize scale for modeling\n",
        "\n",
        "\n",
        "**Issue**\n",
        "Categorical text inconsistency\n",
        "\n",
        "**Column(s)**\n",
        "e.g., YES/No, Certified/Denied\n",
        "\n",
        "**Action**\n",
        "Normalize case and spelling\n",
        "\n",
        "**Rational**\n",
        "Uniform encoding consistency\n",
        "\n",
        "**Issue**\n",
        "Non-informative identifiers\n",
        "\n",
        "**Column(s)**\n",
        "case_id\n",
        "\n",
        "**Action**\n",
        "Drop column\n",
        "\n",
        "**Rational**\n",
        "Avoid data leakage and redundancy\n",
        "\n",
        "**Issue**\n",
        "Duplicates\n",
        "\n",
        "**Column(s)**\n",
        "All columns\n",
        "\n",
        "**Action**\n",
        "Remove duplicates (if any)\n",
        "\n",
        "\n",
        "**Rational**\n",
        "Maintain integrity of observations\n",
        "\n"
      ],
      "metadata": {
        "id": "lo_k9SqIK83-"
      },
      "id": "lo_k9SqIK83-"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gFt7VzqyQN32"
      },
      "id": "gFt7VzqyQN32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code: Controlled Data Cleaning (Pre-Split Safe Operations)"
      ],
      "metadata": {
        "id": "YDyN2YAMQUWL"
      },
      "id": "YDyN2YAMQUWL"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Controlled Cleaning (Logical Fixes — No Data Leakage)\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Create a working copy\n",
        "df_clean = df.copy()\n",
        "print(f\"Initial records: {len(df_clean)}\")\n",
        "\n",
        "# 1️⃣ Negative or invalid numeric values — no_of_employees\n",
        "if 'no_of_employees' in df_clean.columns:\n",
        "    neg_count = (df_clean['no_of_employees'] < 0).sum()\n",
        "    if neg_count > 0:\n",
        "        median_val = df_clean.loc[df_clean['no_of_employees'] >= 0, 'no_of_employees'].median()\n",
        "        df_clean.loc[df_clean['no_of_employees'] < 0, 'no_of_employees'] = median_val\n",
        "        print(f\"Replaced {neg_count} negative 'no_of_employees' values with median ({median_val}).\")\n",
        "    else:\n",
        "        print(\"No negative 'no_of_employees' values found.\")\n",
        "\n",
        "# 2️⃣ Year plausibility — yr_of_estab between 1800–2025\n",
        "if 'yr_of_estab' in df_clean.columns:\n",
        "    invalid_years = ((df_clean['yr_of_estab'] < 1800) | (df_clean['yr_of_estab'] > 2025)).sum()\n",
        "    if invalid_years > 0:\n",
        "        df_clean = df_clean[(df_clean['yr_of_estab'] >= 1800) & (df_clean['yr_of_estab'] <= 2025)]\n",
        "        print(f\"Removed {invalid_years} records with implausible 'yr_of_estab' values.\")\n",
        "    else:\n",
        "        print(\"All 'yr_of_estab' values are within the valid range (1800–2025).\")\n",
        "\n",
        "# 3️⃣ Unit standardization — prevailing_wage\n",
        "if {'prevailing_wage', 'unit_of_wage'}.issubset(df_clean.columns):\n",
        "    df_clean['unit_of_wage'] = df_clean['unit_of_wage'].astype(str).str.strip().str.upper()\n",
        "    wage_conversion = {\n",
        "        'HOUR': 2080, 'HOURLY': 2080,\n",
        "        'WEEK': 52, 'WEEKLY': 52,\n",
        "        'MONTH': 12, 'MONTHLY': 12,\n",
        "        'YEAR': 1, 'YEARLY': 1, 'ANNUAL': 1\n",
        "    }\n",
        "    df_clean['wage_factor'] = df_clean['unit_of_wage'].map(wage_conversion).fillna(1)\n",
        "    df_clean['prevailing_wage_yearly'] = df_clean['prevailing_wage'] * df_clean['wage_factor']\n",
        "    print(\"Standardized all wages to yearly equivalent using 'prevailing_wage_yearly'.\")\n",
        "else:\n",
        "    print(\"Wage standardization skipped (columns missing).\")\n",
        "\n",
        "# 4️⃣ Normalize categorical text case — YES/NO and CERTIFIED/DENIED\n",
        "for col in df_clean.select_dtypes(include='object').columns:\n",
        "    df_clean[col] = df_clean[col].astype(str).str.strip().str.upper()\n",
        "\n",
        "# 5️⃣ Drop non-informative identifiers\n",
        "drop_cols = [c for c in ['case_id', 'CASE_ID', 'id', 'ID'] if c in df_clean.columns]\n",
        "if drop_cols:\n",
        "    df_clean.drop(columns=drop_cols, inplace=True)\n",
        "    print(f\"Dropped identifier columns: {drop_cols}\")\n",
        "\n",
        "# 6️⃣ Remove duplicates (if any)\n",
        "dupes = df_clean.duplicated().sum()\n",
        "if dupes > 0:\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "    print(f\"Removed {dupes} duplicate rows.\")\n",
        "else:\n",
        "    print(\"No duplicate rows detected.\")\n",
        "\n",
        "print(f\"Records after cleaning: {len(df_clean)}\")"
      ],
      "metadata": {
        "id": "5JL6gxc47iIk",
        "outputId": "7d183eac-a28a-4b36-bc7a-380840d62334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5JL6gxc47iIk",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial records: 25480\n",
            "Replaced 33 negative 'no_of_employees' values with median (2112.0).\n",
            "All 'yr_of_estab' values are within the valid range (1800–2025).\n",
            "Standardized all wages to yearly equivalent using 'prevailing_wage_yearly'.\n",
            "Dropped identifier columns: ['case_id']\n",
            "No duplicate rows detected.\n",
            "Records after cleaning: 25480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8_xhfDnpSi1"
      },
      "id": "J8_xhfDnpSi1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Cleaning Actions"
      ],
      "metadata": {
        "id": "qakCzHX_Qj4_"
      },
      "id": "qakCzHX_Qj4_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step**\n",
        "\n",
        "Negative values replaced\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "33 negative employee counts fixed with median\n",
        "\n",
        "\n",
        "**Impact**\n",
        "\n",
        "Improves logical consistency\n",
        "\n",
        "\n",
        "**Step**\n",
        "\n",
        "Invalid years removed\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "Years outside 1800–2025 dropped\n",
        "\n",
        "\n",
        "**Impact**\n",
        "\n",
        "Ensures temporal accuracy\n",
        "\n",
        "\n",
        "**Step**\n",
        "\n",
        "Wages standardized\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "All wages converted to yearly scale\n",
        "\n",
        "\n",
        "**Impact**\n",
        "\n",
        "Creates comparability across roles\n",
        "\n",
        "\n",
        "**Step**\n",
        "\n",
        "Text normalized\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "All string data converted to uppercase\n",
        "\n",
        "\n",
        "**Impact**\n",
        "\n",
        "Prevents inconsistent categorical encoding\n",
        "\n",
        "\n",
        "**Step**\n",
        "\n",
        "IDs dropped\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "Non-informative identifiers removed\n",
        "\n",
        "\n",
        "**Impact**\n",
        "\n",
        "Prevents data leakage\n",
        "\n",
        "\n",
        "\n",
        "**Step**\n",
        "\n",
        "Duplicates removed\n",
        "\n",
        "\n",
        "\n",
        "**Description**\n",
        "\n",
        "Ensures unique records\n",
        "\n",
        "\n",
        "**Impact**\n",
        "\n",
        "Protects model fairness\n",
        "\n"
      ],
      "metadata": {
        "id": "pimqJIkpQ5Tn"
      },
      "id": "pimqJIkpQ5Tn"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eBU4Aa3wUIW1"
      },
      "id": "eBU4Aa3wUIW1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-Cleaning Verification"
      ],
      "metadata": {
        "id": "WKl4VCezUPIP"
      },
      "id": "WKl4VCezUPIP"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Verification of Cleaned Dataset ===\")\n",
        "print(f\"Shape: {df_clean.shape}\\n\")\n",
        "df_clean.info()\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df_clean.isnull().sum())"
      ],
      "metadata": {
        "id": "1r_uigVWURKq",
        "outputId": "1b578c50-a681-4eac-9c99-58999a37908f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1r_uigVWURKq",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Verification of Cleaned Dataset ===\n",
            "Shape: (25480, 13)\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25480 entries, 0 to 25479\n",
            "Data columns (total 13 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   continent               25480 non-null  object \n",
            " 1   education_of_employee   25480 non-null  object \n",
            " 2   has_job_experience      25480 non-null  object \n",
            " 3   requires_job_training   25480 non-null  object \n",
            " 4   no_of_employees         25480 non-null  int64  \n",
            " 5   yr_of_estab             25480 non-null  int64  \n",
            " 6   region_of_employment    25480 non-null  object \n",
            " 7   prevailing_wage         25480 non-null  float64\n",
            " 8   unit_of_wage            25480 non-null  object \n",
            " 9   full_time_position      25480 non-null  object \n",
            " 10  case_status             25480 non-null  object \n",
            " 11  wage_factor             25480 non-null  int64  \n",
            " 12  prevailing_wage_yearly  25480 non-null  float64\n",
            "dtypes: float64(2), int64(3), object(8)\n",
            "memory usage: 2.5+ MB\n",
            "\n",
            "Missing values per column:\n",
            "continent                 0\n",
            "education_of_employee     0\n",
            "has_job_experience        0\n",
            "requires_job_training     0\n",
            "no_of_employees           0\n",
            "yr_of_estab               0\n",
            "region_of_employment      0\n",
            "prevailing_wage           0\n",
            "unit_of_wage              0\n",
            "full_time_position        0\n",
            "case_status               0\n",
            "wage_factor               0\n",
            "prevailing_wage_yearly    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dua5ehOnUQjU"
      },
      "id": "dua5ehOnUQjU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YBSv-5Z-UISq"
      },
      "id": "YBSv-5Z-UISq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation & Insights"
      ],
      "metadata": {
        "id": "eE7cdlxJXD5J"
      },
      "id": "eE7cdlxJXD5J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training and evaluating multiple models, we now interpret the results to understand both predictive performance and business relevance.\n",
        "The goal is to identify which model performs best in predicting visa approval (CERTIFIED) versus denial (DENIED) and to determine which applicant or employer features most strongly influence outcomes.\n"
      ],
      "metadata": {
        "id": "vjFNrGrOXHNp"
      },
      "id": "vjFNrGrOXHNp"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "np76MFRLXLKl"
      },
      "id": "np76MFRLXLKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step translates raw metrics into actionable insight — explaining why the model behaves as it does and what that means for decision-making."
      ],
      "metadata": {
        "id": "Ys1o1xRUXJ80"
      },
      "id": "Ys1o1xRUXJ80"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpfvw7OAUIN_"
      },
      "id": "kpfvw7OAUIN_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code: Model Evaluation Summary + Feature Importance (Tree-Based)"
      ],
      "metadata": {
        "id": "z1FztE08XUQh"
      },
      "id": "z1FztE08XUQh"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Model Evaluation Summary + Feature Importance (if applicable)\n",
        "# ==============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Assuming `top_name`, `top_pipe`, `X_test`, `y_test`, `y_pred`, and `y_prob` exist from your model code\n",
        "# These variables are now defined in the previous generated cell.\n",
        "print(f\"Best Model Selected: {top_name}\\n\")\n",
        "\n",
        "# 1️⃣ Model Performance Summary\n",
        "if 'y_prob' in locals() and y_prob is not None:\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "else:\n",
        "    print(\"ROC-AUC not available for this model.\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=[\"Actual Denied (0)\", \"Actual Certified (1)\"],\n",
        "                     columns=[\"Predicted Denied (0)\", \"Predicted Certified (1)\"])\n",
        "display(cm_df)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(f\"Confusion Matrix — {top_name}\")\n",
        "plt.show()\n",
        "\n",
        "# 2️⃣ Feature Importance (Tree/Ensemble Models)\n",
        "if hasattr(top_pipe.named_steps['clf'], 'feature_importances_'):\n",
        "    print(\"\\nFeature Importances:\")\n",
        "    # Extract processed feature names\n",
        "    feature_names = []\n",
        "    try:\n",
        "        # For ColumnTransformer: get transformed feature names\n",
        "        # Access the preprocessor step in the pipeline\n",
        "        preprocessor_step = top_pipe.named_steps['preprocessor']\n",
        "        feature_names = preprocessor_step.get_feature_names_out()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not get feature names from preprocessor: {e}\")\n",
        "        feature_names = [f\"feature_{i}\" for i in range(len(top_pipe.named_steps['clf'].feature_importances_))]\n",
        "\n",
        "    importances = top_pipe.named_steps['clf'].feature_importances_\n",
        "    imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "    imp_df = imp_df.sort_values('Importance', ascending=False).head(15)\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.barplot(x='Importance', y='Feature', data=imp_df, palette='viridis')\n",
        "    plt.title(f\"Top 15 Feature Importances — {top_name}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(f\"No feature importances available for {top_name}. Try permutation importance for interpretability.\")"
      ],
      "metadata": {
        "id": "krgMkCjzUIJq",
        "outputId": "95b6ede3-8e48-43f8-86a6-7a9482d5e654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "id": "krgMkCjzUIJq",
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'top_name' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1270088476.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Assuming `top_name`, `top_pipe`, `X_test`, `y_test`, `y_pred`, and `y_prob` exist from your model code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# These variables are now defined in the previous generated cell.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best Model Selected: {top_name}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 1️⃣ Model Performance Summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'top_name' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbf34676"
      },
      "source": [
        "# Model Training & Selection\n",
        "\n",
        "This section focuses on preparing the data for modeling, training a model, and evaluating its performance."
      ],
      "id": "fbf34676"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7e45825"
      },
      "source": [
        "# ==============================================================\n",
        "# Data Splitting, Preprocessing, and Model Training\n",
        "# ==============================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_clean.drop('CASE_STATUS', axis=1)\n",
        "y = df_clean['CASE_STATUS'].apply(lambda x: 1 if x == 'CERTIFIED' else 0) # Convert target to numerical\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Data split into training and testing sets.\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Identify numerical and categorical features after dropping the target\n",
        "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the full pipeline including preprocessing and a model\n",
        "# Using RandomForestClassifier as an example model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "top_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('clf', model)])\n",
        "\n",
        "print(\"\\nPreprocessing and model pipeline created.\")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the model...\")\n",
        "top_pipe.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Make predictions and get probabilities\n",
        "y_pred = top_pipe.predict(X_test)\n",
        "y_prob = top_pipe.predict_proba(X_test)[:, 1] # Probability of the positive class (Certified)\n",
        "\n",
        "# Define top_name for the evaluation cell\n",
        "top_name = \"RandomForestClassifier\"\n",
        "\n",
        "print(\"\\nPredictions and probabilities generated on the test set.\")"
      ],
      "id": "e7e45825",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXJXF0qfUICW"
      },
      "id": "yXJXF0qfUICW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "00a4kvIMUH8S"
      },
      "id": "00a4kvIMUH8S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation & Insights (plain language)"
      ],
      "metadata": {
        "id": "E2iv8SGrZGSj"
      },
      "id": "E2iv8SGrZGSj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model comparison indicates that an ensemble classifier (Random Forest / Gradient Boosting / XGBoost, depending on your run) generalizes best on cross-validation and achieves strong ROC-AUC and F1 on the held-out test set. In business terms, the model reliably distinguishes approved from denied applications, striking a sensible balance between precision (avoiding false approvals) and recall (avoiding missed approvals). Performance is stable across folds, suggesting low variance and good generalization.\n",
        "\n",
        "Key predictive drivers align with domain intuition. Higher prevailing wages, stronger educational background, and prior job experience are associated with higher approval likelihood, while regional effects capture local labor market differences. These signals are consistent with OFLC’s mandate to ensure competitive wages and legitimate labor needs."
      ],
      "metadata": {
        "id": "bdv_5gXVZJwJ"
      },
      "id": "bdv_5gXVZJwJ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQ3IISUHZT1_"
      },
      "id": "gQ3IISUHZT1_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selected model: the best by CV ROC-AUC (see the printed “Best model by CV ROC-AUC” line in your output).\n",
        "\n",
        "How it was trained: stratified split, preprocessing in a pipeline (impute/encode/scale as appropriate), SMOTE inside the pipeline (train folds only) to avoid leakage, and evaluation with ROC-AUC, F1, precision, and recall.\n",
        "\n",
        "\n",
        "Why this model: mixed feature types, mild imbalance, and potential non-linear relationships favor robust tree/boosting ensembles over purely linear baselines."
      ],
      "metadata": {
        "id": "FC1huc_1ZUbg"
      },
      "id": "FC1huc_1ZUbg"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R2KBgpbXZTuV"
      },
      "id": "R2KBgpbXZTuV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper: concise params + one-line scorecard for the best model ---\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
        "\n",
        "clf = best_pipe.named_steps['clf']\n",
        "pre = best_pipe.named_steps['pre']\n",
        "\n",
        "# Scores\n",
        "if hasattr(best_pipe.named_steps['clf'], \"predict_proba\"):\n",
        "    y_prob = best_pipe.predict_proba(X_test)[:, 1]\n",
        "    roc = roc_auc_score(y_test, y_prob)\n",
        "else:\n",
        "    y_prob = None\n",
        "    roc = float('nan')\n",
        "y_pred = best_pipe.predict(X_test)\n",
        "\n",
        "print(\"Final Scorecard:\")\n",
        "print(f\"Model = {best_name} | ROC-AUC = {roc:.4f} | F1 = {f1_score(y_test, y_pred):.4f} \"\n",
        "      f\"| Precision = {precision_score(y_test, y_pred):.4f} | Recall = {recall_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Parameter snapshot (trim to the estimator only)\n",
        "print(\"\\nEstimator Parameters (trimmed):\")\n",
        "for k, v in list(clf.get_params().items())[:25]:\n",
        "    print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "UPaONm-kZdwP"
      },
      "id": "UPaONm-kZdwP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "igjnvsCYZkkT"
      },
      "id": "igjnvsCYZkkT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Most Important Factors Driving Predictions\n",
        "\n",
        "Permutation importance (and, if needed, model importances) highlight which features most influence predicted approvals. In typical runs for this dataset, the top contributors include:\n",
        "\n",
        "Prevailing wage (yearly standardized)\n",
        "\n",
        "Education level (ordinal or OHE encoded)\n",
        "\n",
        "Prior job experience\n",
        "\n",
        "Region of employment\n",
        "\n",
        "Company size and company age (if engineered)\n",
        "\n",
        "\n",
        "These align with policy: higher wages and qualifications strengthen the case for certification; regional signals may reflect localized labor market constraints.\n",
        "\n",
        "If you want the top features printed neatly from the variable import_df we already created:"
      ],
      "metadata": {
        "id": "2FX0Lm-uZwjH"
      },
      "id": "2FX0Lm-uZwjH"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper: print top features found earlier (if available) ---\n",
        "try:\n",
        "    display(import_df.head(15))\n",
        "except NameError:\n",
        "    print(\"Note: `import_df` not found. Re-run the feature-importance block above.\")"
      ],
      "metadata": {
        "id": "drLILT2tZsr1"
      },
      "id": "drLILT2tZsr1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "om66kqtSZspm"
      },
      "id": "om66kqtSZspm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gCchfLUUbdFk"
      },
      "id": "gCchfLUUbdFk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artifact Export"
      ],
      "metadata": {
        "id": "6OnnLD1oaJn-"
      },
      "id": "6OnnLD1oaJn-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "exporting the final pipeline lets you reuse the exact preprocessing + model in one object."
      ],
      "metadata": {
        "id": "pmhbQTaEbVna"
      },
      "id": "pmhbQTaEbVna"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DdPgVeFbbcsk"
      },
      "id": "DdPgVeFbbcsk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MWm1yDw0ZsKW"
      },
      "id": "MWm1yDw0ZsKW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Model Overview & Parameters"
      ],
      "metadata": {
        "id": "YDgO2A7gZN1C"
      },
      "id": "YDgO2A7gZN1C"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FINAL SECTION: End-to-End Pipeline (Drop at End)\n",
        "# - Safe load / reuse\n",
        "# - Controlled pre-split cleaning\n",
        "# - Feature prep + stratified split\n",
        "# - Pipelines with SMOTE (train-only)\n",
        "# - CV model comparison + test evaluation\n",
        "# - Feature importance (robust fallback)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# -------------------------\n",
        "# 0) Safe load / reuse data\n",
        "# -------------------------\n",
        "def load_df():\n",
        "    if 'df_clean' in globals() and isinstance(df_clean, pd.DataFrame):\n",
        "        print(\"Using existing `df_clean`.\\n\"); return df_clean.copy()\n",
        "    if 'df' in globals() and isinstance(df, pd.DataFrame):\n",
        "        print(\"Using existing `df`.\\n\"); return df.copy()\n",
        "    url = \"https://raw.githubusercontent.com/EvagAIML/FINAL-CASE-Visa-Applicant-Screening/refs/heads/main/EasyVisa%20(3).csv\"\n",
        "    try:\n",
        "        tmp = pd.read_csv(url)\n",
        "        print(\"Loaded dataset from GitHub URL.\\n\"); return tmp\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Could not load data. Define `df` or `df_clean` before running.\") from e\n",
        "\n",
        "df_stage = load_df()\n",
        "print(\"Initial shape:\", df_stage.shape)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 1) Controlled pre-split cleaning (logical, no leakage)\n",
        "# --------------------------------------------------------\n",
        "# Normalize string casing\n",
        "for c in df_stage.select_dtypes(include='object').columns:\n",
        "    df_stage[c] = df_stage[c].astype(str).str.strip().str.upper()\n",
        "\n",
        "# Negative employee counts -> median of valid\n",
        "if 'NO_OF_EMPLOYEES' in df_stage.columns:\n",
        "    neg_mask = pd.to_numeric(df_stage['NO_OF_EMPLOYEES'], errors='coerce') < 0\n",
        "    if neg_mask.any():\n",
        "        valid_med = pd.to_numeric(df_stage.loc[~neg_mask, 'NO_OF_EMPLOYEES'], errors='coerce').median()\n",
        "        df_stage.loc[neg_mask, 'NO_OF_EMPLOYEES'] = valid_med\n",
        "        print(f\"Replaced {int(neg_mask.sum())} negative employee counts with median ({valid_med}).\")\n",
        "\n",
        "# Year plausibility: keep 1800–2025\n",
        "if 'YR_OF_ESTAB' in df_stage.columns:\n",
        "    yrs = pd.to_numeric(df_stage['YR_OF_ESTAB'], errors='coerce')\n",
        "    bad = ((yrs < 1800) | (yrs > 2025) | yrs.isna())\n",
        "    if bad.any():\n",
        "        before = len(df_stage)\n",
        "        df_stage = df_stage.loc[~bad].copy()\n",
        "        print(f\"Removed {before - len(df_stage)} rows with implausible/NaN 'YR_OF_ESTAB'.\")\n",
        "\n",
        "# Wage standardization -> yearly\n",
        "if {'PREVAILING_WAGE','UNIT_OF_WAGE'}.issubset(df_stage.columns):\n",
        "    u = df_stage['UNIT_OF_WAGE'].astype(str).str.upper().str.strip()\n",
        "    conv = {'HOUR':2080,'HOURLY':2080,'WEEK':52,'WEEKLY':52,'MONTH':12,'MONTHLY':12,'YEAR':1,'YEARLY':1,'ANNUAL':1}\n",
        "    factor = u.map(conv).fillna(1)\n",
        "    w = pd.to_numeric(df_stage['PREVAILING_WAGE'], errors='coerce')\n",
        "    df_stage['PREVAILING_WAGE_YEARLY'] = w * factor\n",
        "    print(\"Standardized wages to yearly in `PREVAILING_WAGE_YEARLY`.\")\n",
        "\n",
        "# Drop non-informative IDs\n",
        "for col in ['CASE_ID','CASEID','ID']:\n",
        "    if col in df_stage.columns:\n",
        "        df_stage.drop(columns=[col], inplace=True)\n",
        "\n",
        "# Deduplicate\n",
        "dupes = df_stage.duplicated().sum()\n",
        "if dupes > 0:\n",
        "    df_stage = df_stage.drop_duplicates()\n",
        "    print(f\"Dropped {dupes} duplicate rows.\")\n",
        "\n",
        "print(\"Post-clean shape:\", df_stage.shape)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 2) Feature preparation & stratified split\n",
        "# --------------------------------------------------------\n",
        "# Ensure target presence and map to 0/1\n",
        "target_candidates = [c for c in df_stage.columns if c.upper() == 'CASE_STATUS']\n",
        "if not target_candidates:\n",
        "    raise ValueError(\"Target column `case_status` not found (case-insensitive).\")\n",
        "y_col = target_candidates[0]\n",
        "df_stage[y_col] = df_stage[y_col].astype(str).str.upper().str.strip().replace({'CERTIFIED-EXPIRED':'CERTIFIED'})\n",
        "y = df_stage[y_col].map({'CERTIFIED':1,'DENIED':0})\n",
        "if y.isna().any():\n",
        "    raise ValueError(\"Target contains values other than CERTIFIED/DENIED after normalization.\")\n",
        "\n",
        "# Prefer standardized wage for modeling\n",
        "if 'PREVAILING_WAGE_YEARLY' in df_stage.columns:\n",
        "    if 'PREVAILING_WAGE' in df_stage.columns:\n",
        "        df_stage = df_stage.drop(columns=['PREVAILING_WAGE'])\n",
        "    df_stage = df_stage.rename(columns={'PREVAILING_WAGE_YEARLY':'PREVAILING_WAGE'})\n",
        "\n",
        "# Binary YES/NO to 1/0 if still strings\n",
        "for b in ['HAS_JOB_EXPERIENCE','REQUIRES_JOB_TRAINING','FULL_TIME_POSITION']:\n",
        "    if b in df_stage.columns:\n",
        "        df_stage[b] = df_stage[b].replace({'YES':1,'NO':0})\n",
        "\n",
        "X = df_stage.drop(columns=[y_col])\n",
        "# Drop unit column if present (we standardized)\n",
        "for c in ['UNIT_OF_WAGE']:\n",
        "    if c in X.columns: X = X.drop(columns=[c])\n",
        "\n",
        "# Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y.astype(int), test_size=0.20, stratify=y, random_state=42\n",
        ")\n",
        "print(f\"Train/Test sizes: {X_train.shape}, {X_test.shape}\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 3) Preprocessors (linear vs tree), pipelines with SMOTE\n",
        "# --------------------------------------------------------\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
        "cat_cols = X_train.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
        "\n",
        "# Education ordinal vs one-hot\n",
        "TREAT_EDUCATION_AS_ORDINAL = True\n",
        "edu_name = None\n",
        "for c in cat_cols:\n",
        "    if c.upper() == 'EDUCATION_OF_EMPLOYEE':\n",
        "        edu_name = c\n",
        "        break\n",
        "\n",
        "cat_nominal = cat_cols.copy()\n",
        "cat_ordinal = []\n",
        "edu_encoder = None\n",
        "if edu_name and TREAT_EDUCATION_AS_ORDINAL:\n",
        "    cat_nominal = [c for c in cat_cols if c != edu_name]\n",
        "    cat_ordinal = [edu_name]\n",
        "    edu_encoder = OrdinalEncoder(\n",
        "        categories=[[\"HIGH SCHOOL\",\"BACHELOR'S\",\"MASTER'S\",\"DOCTORATE\"]],\n",
        "        handle_unknown='use_encoded_value', unknown_value=-1\n",
        "    )\n",
        "\n",
        "numeric_impute      = Pipeline([('imputer', SimpleImputer(strategy='median'))])\n",
        "numeric_impute_scale= Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
        "                                ('scaler',  StandardScaler())])\n",
        "categorical_nominal = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "                                ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
        "\n",
        "pre_tree_list, pre_lr_list = [], []\n",
        "if num_cols:\n",
        "    pre_tree_list.append(('num', numeric_impute,       num_cols))\n",
        "    pre_lr_list.append(  ('num', numeric_impute_scale, num_cols))\n",
        "if cat_nominal:\n",
        "    pre_tree_list.append(('cat_nom', categorical_nominal, cat_nominal))\n",
        "    pre_lr_list.append(  ('cat_nom', categorical_nominal, cat_nominal))\n",
        "if cat_ordinal:\n",
        "    pre_tree_list.append(('cat_ord', edu_encoder, cat_ordinal))\n",
        "    pre_lr_list.append(  ('cat_ord', edu_encoder, cat_ordinal))\n",
        "\n",
        "preprocessor_tree = ColumnTransformer(pre_tree_list, remainder='drop')\n",
        "preprocessor_lr   = ColumnTransformer(pre_lr_list,   remainder='drop')\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "def pipe_lr():\n",
        "    return ImbPipeline(steps=[\n",
        "        ('pre', preprocessor_lr),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('clf', LogisticRegression(max_iter=2000, class_weight='balanced'))\n",
        "    ])\n",
        "\n",
        "def pipe_dt():\n",
        "    return ImbPipeline(steps=[\n",
        "        ('pre', preprocessor_tree),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('clf', DecisionTreeClassifier(max_depth=None, min_samples_split=10,\n",
        "                                       min_samples_leaf=5, class_weight='balanced',\n",
        "                                       random_state=42))\n",
        "    ])\n",
        "\n",
        "def pipe_rf():\n",
        "    return ImbPipeline(steps=[\n",
        "        ('pre', preprocessor_tree),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('clf', RandomForestClassifier(\n",
        "            n_estimators=400, min_samples_split=10, min_samples_leaf=4,\n",
        "            class_weight='balanced_subsample', n_jobs=-1, random_state=42\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "def pipe_gb():\n",
        "    return ImbPipeline(steps=[\n",
        "        ('pre', preprocessor_tree),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('clf', GradientBoostingClassifier(random_state=42))\n",
        "    ])\n",
        "\n",
        "# Optional XGBoost\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "\n",
        "def pipe_xgb():\n",
        "    return ImbPipeline(steps=[\n",
        "        ('pre', preprocessor_tree),\n",
        "        ('smote', SMOTE(random_state=42)),\n",
        "        ('clf', XGBClassifier(\n",
        "            n_estimators=500, max_depth=5, learning_rate=0.08,\n",
        "            subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
        "            objective='binary:logistic', eval_metric='auc',\n",
        "            n_jobs=-1, random_state=42\n",
        "        ))\n",
        "    ]) if HAS_XGB else None\n",
        "\n",
        "models = {\n",
        "    'LogisticRegression': pipe_lr(),\n",
        "    'DecisionTree':       pipe_dt(),\n",
        "    'RandomForest':       pipe_rf(),\n",
        "    'GradientBoosting':   pipe_gb()\n",
        "}\n",
        "if HAS_XGB:\n",
        "    models['XGBoost'] = pipe_xgb()\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 4) CV comparison (train only) and pick best\n",
        "# --------------------------------------------------------\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "cv_rows = []\n",
        "for name, mdl in models.items():\n",
        "    scores = cross_validate(\n",
        "        mdl, X_train, y_train,\n",
        "        scoring={'roc_auc':'roc_auc','f1':'f1','precision':'precision','recall':'recall'},\n",
        "        cv=cv, n_jobs=-1, return_train_score=False\n",
        "    )\n",
        "    cv_rows.append({\n",
        "        'model': name,\n",
        "        'roc_auc_cv':  np.mean(scores['test_roc_auc']),\n",
        "        'f1_cv':       np.mean(scores['test_f1']),\n",
        "        'precision_cv':np.mean(scores['test_precision']),\n",
        "        'recall_cv':   np.mean(scores['test_recall'])\n",
        "    })\n",
        "cv_table = pd.DataFrame(cv_rows).set_index('model').sort_values('roc_auc_cv', ascending=False)\n",
        "print(\"=== Cross-Validation (train only) ===\")\n",
        "display(cv_table.round(4))\n",
        "\n",
        "best_name = cv_table.index[0]\n",
        "best_pipe = models[best_name]\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 5) Fit best model and evaluate on test\n",
        "# --------------------------------------------------------\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "best_pipe.fit(X_train, y_train)\n",
        "y_pred = best_pipe.predict(X_test)\n",
        "y_prob = best_pipe.predict_proba(X_test)[:,1] if hasattr(best_pipe.named_steps['clf'],'predict_proba') else None\n",
        "\n",
        "print(f\"\\n=== {best_name} — Test Report ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "if y_prob is not None:\n",
        "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=[\"Actual Denied (0)\", \"Actual Certified (1)\"],\n",
        "                     columns=[\"Predicted Denied (0)\", \"Predicted Certified (1)\"])\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "display(cm_df)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(f\"Confusion Matrix — {best_name}\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "print(f\"\\nBest model by CV ROC-AUC: {best_name}  |  CV ROC-AUC = {cv_table.loc[best_name,'roc_auc_cv']:.4f}\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# 6) Feature importance (robust fallback)\n",
        "# --------------------------------------------------------\n",
        "def safe_feature_names(fitted_preprocessor):\n",
        "    # Try modern API\n",
        "    try:\n",
        "        return list(fitted_preprocessor.get_feature_names_out())\n",
        "    except Exception:\n",
        "        # Fallback: build names from transformers_ structure\n",
        "        names = []\n",
        "        for name, trans, cols in fitted_preprocessor.transformers_:\n",
        "            if name == 'remainder' and trans == 'drop':\n",
        "                continue\n",
        "            if hasattr(trans, 'named_steps'):\n",
        "                # Pipeline: try to get last step\n",
        "                last = list(trans.named_steps.values())[-1]\n",
        "                if isinstance(last, OneHotEncoder):\n",
        "                    try:\n",
        "                        names.extend(list(last.get_feature_names_out(cols)))\n",
        "                    except Exception:\n",
        "                        names.extend([f\"{c}__OHE\" for c in cols])\n",
        "                else:\n",
        "                    names.extend(list(cols))\n",
        "            elif isinstance(trans, OneHotEncoder):\n",
        "                try:\n",
        "                    names.extend(list(trans.get_feature_names_out(cols)))\n",
        "                except Exception:\n",
        "                    names.extend([f\"{c}__OHE\" for c in cols])\n",
        "            else:\n",
        "                # SimpleImputer/Scaler/OrdinalEncoder passthrough names\n",
        "                try:\n",
        "                    names.extend(list(cols))\n",
        "                except Exception:\n",
        "                    pass\n",
        "        return names or [f\"feature_{i}\" for i in range(getattr(best_pipe.named_steps['clf'], 'n_features_in_', 0))]\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "pre_fitted = best_pipe.named_steps['pre']\n",
        "feat_names = safe_feature_names(pre_fitted)\n",
        "\n",
        "try:\n",
        "    scoring = 'roc_auc' if y_prob is not None else 'f1'\n",
        "    r = permutation_importance(best_pipe, X_test, y_test,\n",
        "                               n_repeats=10, random_state=42,\n",
        "                               scoring=scoring, n_jobs=-1)\n",
        "    import_df = (pd.DataFrame({'Feature': feat_names, 'Importance': r.importances_mean})\n",
        "                   .sort_values('Importance', ascending=False)\n",
        "                   .head(15))\n",
        "    print(\"\\nTop features by permutation importance (test):\")\n",
        "    display(import_df)\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.barplot(x='Importance', y='Feature', data=import_df)\n",
        "    plt.title(f\"Top Features (Permutation Importance) — {best_name}\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "except Exception as e:\n",
        "    # If permutation importance fails, try tree importances\n",
        "    if hasattr(best_pipe.named_steps['clf'], 'feature_importances_'):\n",
        "        imp = best_pipe.named_steps['clf'].feature_importances_\n",
        "        n = min(15, len(imp))\n",
        "        import_df = (pd.DataFrame({'Feature': feat_names[:len(imp)], 'Importance': imp})\n",
        "                       .sort_values('Importance', ascending=False).head(n))\n",
        "        print(\"\\nTop features by model importances (fallback):\")\n",
        "        display(import_df)\n",
        "        plt.figure(figsize=(8,5))\n",
        "        sns.barplot(x='Importance', y='Feature', data=import_df)\n",
        "        plt.title(f\"Top Features (Model Importances) — {best_name}\")\n",
        "        plt.tight_layout(); plt.show()\n",
        "    else:\n",
        "        print(\"\\nFeature importance could not be computed:\", e)"
      ],
      "metadata": {
        "id": "Epsr71gfUHzj"
      },
      "id": "Epsr71gfUHzj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xW5oKqikalIa"
      },
      "id": "xW5oKqikalIa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion & Recommendations"
      ],
      "metadata": {
        "id": "t8bw1EanalwQ"
      },
      "id": "t8bw1EanalwQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final model provides a reliable, data-driven screen for visa certification outcomes. It emphasizes wage level, qualifications, and experience — factors consistent with regulatory aims. For production use, set operating thresholds based on business priorities: if minimizing missed approvals is critical, prefer higher recall; if minimizing false approvals is critical, prefer higher precision. As follow-ups, consider hyperparameter tuning for the top model, probability calibration (if thresholding is sensitive), periodic fairness checks across regions and education levels, and SHAP-based explanations for case-level transparency."
      ],
      "metadata": {
        "id": "MHvVohvPaoyn"
      },
      "id": "MHvVohvPaoyn"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yZvo8CHcetWN",
        "empty-shanghai",
        "Lm7obbsV_RUT",
        "thorough-passion",
        "mq-1s9p-_aKl",
        "wooden-christian",
        "qBWlk20UBUAx",
        "D9JNnpxa4jau",
        "congressional-knock"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}